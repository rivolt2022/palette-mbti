"""
ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ í–¥ìƒëœ ëª¨ë¸
ì–¼êµ´ íŠ¹ì§•ì˜ ë‹¤ì–‘í•œ ì¡°í•©ìœ¼ë¡œë¶€í„° MBTI ì˜ˆì¸¡ì— ì í•©í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´
"""

import json
import numpy as np
import tensorflow as tf
from tensorflow import keras
import os
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def load_diverse_face_color_data():
    """ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ë°ì´í„° ë¡œë“œ"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(script_dir, "..", "public", "data", "diverse-face-color", "training-data.json")
    data_path = os.path.normpath(data_path)
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    X = np.array(data['X'], dtype=np.float32)
    y = np.array(data['y'], dtype=np.float32)
    metadata = data['metadata']
    
    print(f"   ì…ë ¥ ì°¨ì›: {X.shape[1]} (descriptor 128 + íŠ¹ì§• 15 + ëœë¤ 5)")
    print(f"   ì¶œë ¥ ì°¨ì›: {y.shape[1]}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")
    
    return X, y, metadata

def analyze_color_diversity(y, metadata):
    """ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„"""
    print("\nğŸŒˆ ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„:")
    
    # ìƒ‰ìƒ ì¹´í…Œê³ ë¦¬ ë¶„í¬
    categories = {}
    for item in metadata:
        category = item['colorCategory']
        categories[category] = categories.get(category, 0) + 1
    
    print("   ìƒ‰ìƒ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    for category, count in categories.items():
        print(f"     {category}: {count}ê°œ ({count/len(metadata)*100:.1f}%)")
    
    # ìƒ‰ìƒ íŠ¹ì„± ë¶„ì„
    brightness_values = [item['colorCharacteristics']['brightness'] for item in metadata]
    saturation_values = [item['colorCharacteristics']['saturation'] for item in metadata]
    temperature_values = [item['colorCharacteristics']['temperature'] for item in metadata]
    
    print(f"   ë°ê¸° ë²”ìœ„: {min(brightness_values):.3f} ~ {max(brightness_values):.3f}")
    print(f"   ì±„ë„ ë²”ìœ„: {min(saturation_values):.3f} ~ {max(saturation_values):.3f}")
    print(f"   ìƒ‰ì˜¨ë„ ë²”ìœ„: {min(temperature_values):.3f} ~ {max(temperature_values):.3f}")
    
    # ê³ ìœ  ìƒ‰ìƒ ìˆ˜ ê³„ì‚°
    unique_colors = set()
    for item in metadata:
        for color in item['colors']:
            unique_colors.add(color)
    
    print(f"   ê³ ìœ  ìƒ‰ìƒ ìˆ˜: {len(unique_colors)}ê°œ")

def create_enhanced_diverse_model(input_dim=148, output_dim=15):
    """ë‹¤ì–‘í•œ ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ í–¥ìƒëœ ëª¨ë¸"""
    model = keras.Sequential([
        # ì…ë ¥ì¸µ: 148ì°¨ì›
        keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.4),
        
        # ì€ë‹‰ì¸µ 1: ì–¼êµ´ íŠ¹ì§• ì²˜ë¦¬
        keras.layers.Dense(128, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.3),
        
        # ì€ë‹‰ì¸µ 2: ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµ
        keras.layers.Dense(64, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        
        # ì€ë‹‰ì¸µ 3: ìƒ‰ìƒ ì¡°í•© í•™ìŠµ
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dropout(0.1),
        
        # ì¶œë ¥ì¸µ: 15ì°¨ì› RGB ë²¡í„°
        keras.layers.Dense(output_dim, activation='sigmoid')
    ])
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì½œë°±ìœ¼ë¡œ ì²˜ë¦¬)
    optimizer = keras.optimizers.Adam(learning_rate=0.001)
    
    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae', 'cosine_similarity']
    )
    
    return model

def create_color_diversity_loss():
    """ìƒ‰ìƒ ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜"""
    def diversity_loss(y_true, y_pred):
        # ê¸°ë³¸ MSE ì†ì‹¤
        mse_loss = tf.keras.losses.mse(y_true, y_pred)
        
        # ìƒ‰ìƒ ë‹¤ì–‘ì„± ì†ì‹¤ (ìƒ‰ìƒ ê°„ ê±°ë¦¬ ìµœëŒ€í™”)
        batch_size = tf.shape(y_pred)[0]
        
        # 5ê°œ ìƒ‰ìƒìœ¼ë¡œ ì¬êµ¬ì„± (ê° ìƒ‰ìƒì€ 3ì°¨ì› RGB)
        colors = tf.reshape(y_pred, (batch_size, 5, 3))
        
        # ìƒ‰ìƒ ê°„ í‰ê·  ê±°ë¦¬ ê³„ì‚°
        color_distances = []
        for i in range(5):
            for j in range(i+1, 5):
                dist = tf.norm(colors[:, i, :] - colors[:, j, :], axis=1)
                color_distances.append(dist)
        
        if color_distances:
            avg_distance = tf.reduce_mean(tf.stack(color_distances, axis=1), axis=1)
            diversity_penalty = tf.exp(-avg_distance)  # ê±°ë¦¬ê°€ ê°€ê¹Œìš°ë©´ í˜ë„í‹°
        else:
            diversity_penalty = 0
        
        # ìµœì¢… ì†ì‹¤ = MSE + ë‹¤ì–‘ì„± í˜ë„í‹°
        return mse_loss + 0.1 * tf.reduce_mean(diversity_penalty)
    
    return diversity_loss

def train_diverse_face_to_color_model(X, y, metadata):
    """ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ í•™ìŠµ"""
    print("ğŸ§  ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")
    
    # ì…ë ¥ ë°ì´í„° ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # ëª¨ë¸ ìƒì„±
    model = create_enhanced_diverse_model()
    
    # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print(f"\nğŸ“‹ ëª¨ë¸ êµ¬ì¡°:")
    model.summary()
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=25,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=15,
            min_lr=1e-6,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            'best_diverse_model.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # í•™ìŠµ
    print("\nğŸš€ í•™ìŠµ ì‹œì‘...")
    history = model.fit(
        X_train_scaled, y_train,
        validation_data=(X_val_scaled, y_val),
        epochs=200,
        batch_size=64,
        callbacks=callbacks,
        verbose=1
    )
    
    # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥
    final_loss = history.history['loss'][-1]
    val_loss = history.history['val_loss'][-1]
    print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    print(f"   í›ˆë ¨ ì†ì‹¤: {final_loss:.6f}")
    print(f"   ê²€ì¦ ì†ì‹¤: {val_loss:.6f}")
    
    # ìƒ‰ìƒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸
    test_color_diversity(model, X_val_scaled, y_val)
    
    return model, scaler

def test_color_diversity(model, X_val, y_val):
    """ìƒ‰ìƒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¨ ìƒ‰ìƒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸:")
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = model.predict(X_val[:100])  # 100ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    
    # ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„
    unique_colors = set()
    color_distances = []
    
    for pred in predictions:
        # 5ê°œ ìƒ‰ìƒìœ¼ë¡œ ì¬êµ¬ì„±
        colors = pred.reshape(5, 3)
        
        for color in colors:
            # RGBë¥¼ HEXë¡œ ë³€í™˜
            hex_color = rgb_to_hex(color[0], color[1], color[2])
            unique_colors.add(hex_color)
        
        # ìƒ‰ìƒ ê°„ ê±°ë¦¬ ê³„ì‚°
        for i in range(5):
            for j in range(i+1, 5):
                dist = np.linalg.norm(colors[i] - colors[j])
                color_distances.append(dist)
    
    avg_distance = np.mean(color_distances)
    print(f"   ì˜ˆì¸¡ëœ ê³ ìœ  ìƒ‰ìƒ ìˆ˜: {len(unique_colors)}ê°œ")
    print(f"   í‰ê·  ìƒ‰ìƒ ê°„ ê±°ë¦¬: {avg_distance:.3f}")
    print(f"   ìƒ‰ìƒ ë‹¤ì–‘ì„± ì ìˆ˜: {len(unique_colors) / 100 * 100:.1f}%")

def rgb_to_hex(r, g, b):
    """RGB ê°’ì„ HEXë¡œ ë³€í™˜"""
    def to_hex(n):
        hex_val = hex(int(n * 255))[2:]
        return hex_val if len(hex_val) == 2 else f"0{hex_val}"
    
    return f"#{to_hex(r)}{to_hex(g)}{to_hex(b)}"

def save_diverse_model_as_tfjs(model, scaler):
    """ë‹¤ì–‘í•œ ëª¨ë¸ì„ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    script_dir = os.path.dirname(os.path.abspath(__file__))
    model_dir = os.path.join(script_dir, "..", "public", "models", "diverse-face-to-color")
    model_dir = os.path.normpath(model_dir)
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ í›„ ì¬ìƒì„±
    import shutil
    if os.path.exists(model_dir):
        shutil.rmtree(model_dir)
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"ğŸ”„ ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ì„ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì¤‘...")
    
    # 1. ëª¨ë¸ êµ¬ì¡°ë¥¼ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥
    model_topology = model.to_json()
    model_config = json.loads(model_topology)
    
    # InputLayerì˜ batch_shapeë¥¼ inputShapeë¡œ ë³€í™˜
    if 'config' in model_config and 'layers' in model_config['config']:
        for layer in model_config['config']['layers']:
            if (layer.get('module') == 'keras.layers' and 
                layer.get('class_name') == 'InputLayer'):
                layer_config = layer.get('config', {})
                if 'batch_shape' in layer_config:
                    batch_shape = layer_config['batch_shape']
                    if batch_shape and len(batch_shape) > 1:
                        layer_config['inputShape'] = batch_shape[1:]
                    del layer_config['batch_shape']
    
    # 2. ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì €ì¥
    weights = model.get_weights()
    weight_files = []
    
    for i, weight in enumerate(weights):
        weight_file = f"weights_{i}.bin"
        weight_path = os.path.join(model_dir, weight_file)
        weight.astype('float32').tofile(weight_path)
        weight_files.append(weight_file)
    
    # 3. ê°€ì¤‘ì¹˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
    weights_manifest = [{
        "paths": weight_files,
        "weights": []
    }]
    
    # ê° ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ì¶”ê°€
    weight_names = []
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            weight_names.append(f"{layer.name}/kernel")
            if layer.use_bias:
                weight_names.append(f"{layer.name}/bias")
        elif hasattr(layer, 'gamma'):
            # BatchNormalization ë ˆì´ì–´
            weight_names.extend([
                f"{layer.name}/gamma",
                f"{layer.name}/beta", 
                f"{layer.name}/moving_mean",
                f"{layer.name}/moving_variance"
            ])
    
    for i, weight in enumerate(weights):
        weight_name = weight_names[i] if i < len(weight_names) else f"weight_{i}"
        weights_manifest[0]["weights"].append({
            "name": weight_name,
            "shape": list(weight.shape),
            "dtype": "float32"
        })
    
    # 4. model.json íŒŒì¼ ìƒì„±
    model_json = {
        "modelTopology": model_config,
        "weightsManifest": weights_manifest
    }
    
    with open(os.path.join(model_dir, 'model.json'), 'w') as f:
        json.dump(model_json, f, indent=2)
    
    # 5. ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì €ì¥
    scaler_info = {
        'mean': scaler.mean_.tolist(),
        'scale': scaler.scale_.tolist(),
        'n_features_in_': scaler.n_features_in_
    }
    
    with open(os.path.join(model_dir, 'scaler_info.json'), 'w') as f:
        json.dump(scaler_info, f, indent=2)
    
    # 6. ëª¨ë¸ ì •ë³´ ì €ì¥
    model_info = {
        'input_dim': 148,
        'output_dim': 15,
        'description': 'ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸: MBTI ì˜ˆì¸¡ì„ ìœ„í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´',
        'input_breakdown': {
            'face_descriptor': 128,
            'physical_features': 15,
            'random_seed': 5,
            'total': 148
        },
        'features': [
            'diverse_color_palettes', 'face_shape_analysis', 'color_harmony',
            'brightness_variation', 'saturation_diversity', 'temperature_range'
        ],
        'color_categories': [
            'vibrant', 'harmonious', 'cool', 'warm', 'neutral', 'contrast', 'random'
        ],
        'mbti_optimized': True
    }
    
    with open(os.path.join(model_dir, 'model_info.json'), 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ì´ TensorFlow.js í˜•ì‹ìœ¼ë¡œ {model_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   ğŸ“ ìƒì„±ëœ íŒŒì¼: model.json, {len(weight_files)}ê°œ ê°€ì¤‘ì¹˜ íŒŒì¼, scaler_info.json, model_info.json")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸ¨ MBTI ì˜ˆì¸¡ì„ ìœ„í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´")
    print("ğŸ“Š 148ì°¨ì› ì…ë ¥: descriptor(128) + ë¬¼ë¦¬ì  íŠ¹ì§•(15) + ëœë¤ ì‹œë“œ(5)")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        X, y, metadata = load_diverse_face_color_data()
        
        # ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„
        analyze_color_diversity(y, metadata)
        
        # ëª¨ë¸ í•™ìŠµ
        model, scaler = train_diverse_face_to_color_model(X, y, metadata)
        
        # TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥
        save_diverse_model_as_tfjs(model, scaler)
        
        print("\nğŸ‰ ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
        print("ğŸŒ ì´ì œ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("   - model.json: TensorFlow.js í˜¸í™˜ ëª¨ë¸ êµ¬ì¡°")
        print("   - weights_*.bin: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ë“¤")
        print("   - scaler_info.json: ì…ë ¥ ì •ê·œí™” ì •ë³´")
        print("   - model_info.json: ëª¨ë¸ ì •ë³´")
        
    except FileNotFoundError:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € 'npm run generate-diverse-data'ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
    except Exception as error:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error}")

if __name__ == "__main__":
    main()