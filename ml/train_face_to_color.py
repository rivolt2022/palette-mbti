"""
ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ í–¥ìƒëœ ëª¨ë¸
ì–¼êµ´ íŠ¹ì§•ì˜ ë‹¤ì–‘í•œ ì¡°í•©ìœ¼ë¡œë¶€í„° MBTI ì˜ˆì¸¡ì— ì í•©í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´
"""

import json
import numpy as np
import tensorflow as tf
from tensorflow import keras
import os
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def load_diverse_face_color_data():
    """ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ë°ì´í„° ë¡œë“œ"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(script_dir, "..", "public", "data", "diverse-face-color", "training-data.json")
    data_path = os.path.normpath(data_path)
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    X = np.array(data['X'], dtype=np.float32)
    y = np.array(data['y'], dtype=np.float32)
    metadata = data['metadata']
    
    print(f"   ì…ë ¥ ì°¨ì›: {X.shape[1]} (descriptor 128 + íŠ¹ì§• 15 + ëœë¤ 5)")
    print(f"   ì¶œë ¥ ì°¨ì›: {y.shape[1]}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")
    
    # ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë¶„ì„
    print("\nğŸ” ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë¶„ì„:")
    descriptor_data = X[:, :128]
    physical_features = X[:, 128:143]
    random_seeds = X[:, 143:148]
    
    print(f"   ì–¼êµ´ descriptor ë²”ìœ„: {descriptor_data.min():.3f} ~ {descriptor_data.max():.3f}")
    print(f"   ë¬¼ë¦¬ì  íŠ¹ì§• ë²”ìœ„: {physical_features.min():.3f} ~ {physical_features.max():.3f}")
    print(f"   ëœë¤ ì‹œë“œ ë²”ìœ„: {random_seeds.min():.3f} ~ {random_seeds.max():.3f}")
    
    # ê·¹ê°’ ë¶„ì„
    extreme_values = np.sum((physical_features == 0) | (physical_features == 1))
    total_physical = physical_features.size
    print(f"   ë¬¼ë¦¬ì  íŠ¹ì§• ê·¹ê°’ ë¹„ìœ¨: {extreme_values/total_physical*100:.1f}%")
    
    return X, y, metadata

def analyze_color_diversity(y, metadata):
    """ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„"""
    print("\nğŸŒˆ ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„:")
    
    # ìƒ‰ìƒ ì¹´í…Œê³ ë¦¬ ë¶„í¬
    categories = {}
    for item in metadata:
        category = item['colorCategory']
        categories[category] = categories.get(category, 0) + 1
    
    print("   ìƒ‰ìƒ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    for category, count in categories.items():
        print(f"     {category}: {count}ê°œ ({count/len(metadata)*100:.1f}%)")
    
    # ìƒ‰ìƒ íŠ¹ì„± ë¶„ì„
    brightness_values = [item['colorCharacteristics']['brightness'] for item in metadata]
    saturation_values = [item['colorCharacteristics']['saturation'] for item in metadata]
    temperature_values = [item['colorCharacteristics']['temperature'] for item in metadata]
    
    print(f"   ë°ê¸° ë²”ìœ„: {min(brightness_values):.3f} ~ {max(brightness_values):.3f}")
    print(f"   ì±„ë„ ë²”ìœ„: {min(saturation_values):.3f} ~ {max(saturation_values):.3f}")
    print(f"   ìƒ‰ì˜¨ë„ ë²”ìœ„: {min(temperature_values):.3f} ~ {max(temperature_values):.3f}")
    
    # ê³ ìœ  ìƒ‰ìƒ ìˆ˜ ê³„ì‚°
    unique_colors = set()
    for item in metadata:
        for color in item['colors']:
            unique_colors.add(color)
    
    print(f"   ê³ ìœ  ìƒ‰ìƒ ìˆ˜: {len(unique_colors)}ê°œ")

def create_enhanced_diverse_model(input_dim=148, output_dim=15):
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ í–¥ìƒëœ ëª¨ë¸"""
    model = keras.Sequential([
        # ì…ë ¥ì¸µ: 148ì°¨ì› (ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜)
        keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.4),
        
        # ì€ë‹‰ì¸µ 1: ì–¼êµ´ descriptor ì²˜ë¦¬ (128ì°¨ì›)
        keras.layers.Dense(128, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.3),
        
        # ì€ë‹‰ì¸µ 2: ë¬¼ë¦¬ì  íŠ¹ì§• ì²˜ë¦¬ (15ì°¨ì›, ê·¹ê°’ í¬í•¨)
        keras.layers.Dense(64, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),
        
        # ì€ë‹‰ì¸µ 3: ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµ
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dropout(0.1),
        
        # ì¶œë ¥ì¸µ: 15ì°¨ì› RGB ë²¡í„° (0~1 ë²”ìœ„)
        keras.layers.Dense(output_dim, activation='sigmoid')
    ])
    
    # ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” í•™ìŠµë¥ 
    optimizer = keras.optimizers.Adam(learning_rate=0.0008)  # ì•½ê°„ ë‚®ì¶¤
    
    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae', 'cosine_similarity']
    )
    
    return model

def create_improved_diversity_loss():
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜"""
    def improved_diversity_loss(y_true, y_pred):
        # ê¸°ë³¸ MSE ì†ì‹¤
        mse_loss = tf.keras.losses.mse(y_true, y_pred)
        
        # ê·¹ê°’ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì¡°ì • (ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜)
        # ë¬¼ë¦¬ì  íŠ¹ì§•ì˜ ê·¹ê°’(0, 1)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        extreme_value_mask = tf.cast(
            (tf.abs(y_true - 0.0) < 0.01) | (tf.abs(y_true - 1.0) < 0.01), 
            tf.float32
        )
        extreme_weight = 1.0 + 0.5 * extreme_value_mask  # ê·¹ê°’ì— 1.5ë°° ê°€ì¤‘ì¹˜
        weighted_mse = tf.reduce_mean(tf.square(y_true - y_pred) * extreme_weight)
        
        # ìƒ‰ìƒ ë‹¤ì–‘ì„± ì†ì‹¤ (ìƒ‰ìƒ ê°„ ê±°ë¦¬ ìµœëŒ€í™”)
        batch_size = tf.shape(y_pred)[0]
        colors = tf.reshape(y_pred, (batch_size, 5, 3))
        
        # ìƒ‰ìƒ ê°„ í‰ê·  ê±°ë¦¬ ê³„ì‚°
        color_distances = []
        for i in range(5):
            for j in range(i+1, 5):
                dist = tf.norm(colors[:, i, :] - colors[:, j, :], axis=1)
                color_distances.append(dist)
        
        if color_distances:
            avg_distance = tf.reduce_mean(tf.stack(color_distances, axis=1), axis=1)
            diversity_penalty = tf.exp(-avg_distance * 2)  # ë” ê°•í•œ ë‹¤ì–‘ì„± í˜ë„í‹°
        else:
            diversity_penalty = 0
        
        # ìƒ‰ìƒ ëŒ€ë¹„ ì†ì‹¤ (ìƒ‰ìƒ ê°„ ëŒ€ë¹„ ìµœëŒ€í™”)
        color_contrast = tf.reduce_mean(tf.math.reduce_std(colors, axis=1))  # ê° ìƒ‰ìƒì˜ í‘œì¤€í¸ì°¨
        contrast_penalty = tf.exp(-color_contrast * 3)  # ëŒ€ë¹„ê°€ ë‚®ìœ¼ë©´ í˜ë„í‹°
        
        # ìµœì¢… ì†ì‹¤ = ê°€ì¤‘ MSE + ë‹¤ì–‘ì„± í˜ë„í‹° + ëŒ€ë¹„ í˜ë„í‹°
        return weighted_mse + 0.15 * tf.reduce_mean(diversity_penalty) + 0.1 * contrast_penalty
    
    return improved_diversity_loss

def improved_data_preprocessing(X, y):
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ê°œì„ ëœ ì „ì²˜ë¦¬"""
    print("ğŸ”§ ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ì „ì²˜ë¦¬...")
    
    # ê° íŠ¹ì„±ë³„ë¡œ ë‹¤ë¥¸ ì •ê·œí™” ì „ëµ ì ìš©
    X_processed = X.copy()
    
    # ì–¼êµ´ descriptor (0-127): ì´ë¯¸ -0.35~0.35 ë²”ìœ„ë¡œ ì •ê·œí™”ë¨
    descriptor_data = X_processed[:, :128]
    print(f"   ì–¼êµ´ descriptor: ë²”ìœ„ {descriptor_data.min():.3f} ~ {descriptor_data.max():.3f}")
    
    # ë¬¼ë¦¬ì  íŠ¹ì§• (128-142): 0~1 ë²”ìœ„, ê·¹ê°’ í¬í•¨
    physical_features = X_processed[:, 128:143]
    print(f"   ë¬¼ë¦¬ì  íŠ¹ì§•: ë²”ìœ„ {physical_features.min():.3f} ~ {physical_features.max():.3f}")
    
    # ëœë¤ ì‹œë“œ (143-147): 0~1 ë²”ìœ„
    random_seeds = X_processed[:, 143:148]
    print(f"   ëœë¤ ì‹œë“œ: ë²”ìœ„ {random_seeds.min():.3f} ~ {random_seeds.max():.3f}")
    
    # ê° íŠ¹ì„±ë³„ë¡œ ë‹¤ë¥¸ ì •ê·œí™” ì ìš©
    # descriptorëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # ë¬¼ë¦¬ì  íŠ¹ì§•ê³¼ ëœë¤ ì‹œë“œëŠ” 0~1 ë²”ìœ„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    return X_processed, y

def train_diverse_face_to_color_model(X, y, metadata):
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ"""
    print("ğŸ§  ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # ê°œì„ ëœ ì „ì²˜ë¦¬ ì ìš©
    X_processed, y_processed = improved_data_preprocessing(X, y)
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(
        X_processed, y_processed, test_size=0.2, random_state=42
    )
    
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")
    
    # ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì •ê·œí™” (ê° íŠ¹ì„±ë³„ë¡œ ë‹¤ë¥´ê²Œ)
    # descriptorëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # ë¬¼ë¦¬ì  íŠ¹ì§•ê³¼ ëœë¤ ì‹œë“œëŠ” 0~1 ë²”ìœ„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    X_train_scaled = X_train
    X_val_scaled = X_val
    
    # ëª¨ë¸ ìƒì„± (ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©)
    model = create_enhanced_diverse_model()
    
    # ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ë¡œ ëª¨ë¸ ì¬ì»´íŒŒì¼
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0008),
        loss=create_improved_diversity_loss(),
        metrics=['mae', 'cosine_similarity']
    )
    
    # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print(f"\nğŸ“‹ ëª¨ë¸ êµ¬ì¡°:")
    model.summary()
    
    # ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì½œë°± ì„¤ì •
    callbacks = [
        # ì¡°ê¸° ì¢…ë£Œ (ê·¹ê°’ì´ ë§ì•„ì„œ ë” ë§ì€ ì—í¬í¬ í•„ìš”)
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15,  # ë” ë§ì€ ì¸ë‚´ì‹¬
            restore_best_weights=True,
            verbose=1
        ),
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ)
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=8,
            min_lr=1e-6,
            verbose=1
        ),
        
        # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
        keras.callbacks.ModelCheckpoint(
            'best_diverse_face_to_color_model.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜):")
    print(f"   ë°°ì¹˜ í¬ê¸°: 32")
    print(f"   ìµœëŒ€ ì—í¬í¬: 100")
    print(f"   ì¡°ê¸° ì¢…ë£Œ: 15 ì—í¬í¬ ì¸ë‚´ì‹¬")
    print(f"   í•™ìŠµë¥  ê°ì†Œ: 8 ì—í¬í¬ ì¸ë‚´ì‹¬")
    
    # í•™ìŠµ ì‹¤í–‰
    history = model.fit(
        X_train_scaled, y_train,
        validation_data=(X_val_scaled, y_val),
        epochs=100,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    return model, history, X_val_scaled, y_val

def evaluate_model_performance(model, X_val, y_val, metadata):
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    print("\nğŸ“Š ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:")
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred = model.predict(X_val, verbose=0)
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mse = np.mean((y_val - y_pred) ** 2)
    mae = np.mean(np.abs(y_val - y_pred))
    
    print(f"   MSE: {mse:.6f}")
    print(f"   MAE: {mae:.6f}")
    
    # ê·¹ê°’ ì˜ˆì¸¡ ì •í™•ë„ (ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜)
    extreme_mask = (np.abs(y_val - 0.0) < 0.01) | (np.abs(y_val - 1.0) < 0.01)
    extreme_accuracy = np.mean(np.abs(y_val[extreme_mask] - y_pred[extreme_mask]) < 0.1)
    print(f"   ê·¹ê°’ ì˜ˆì¸¡ ì •í™•ë„: {extreme_accuracy:.3f}")
    
    # ìƒ‰ìƒ ë‹¤ì–‘ì„± í‰ê°€
    test_color_diversity(model, X_val, y_val)
    
    return {
        'mse': mse,
        'mae': mae,
        'extreme_accuracy': extreme_accuracy
    }

def test_color_diversity(model, X_val, y_val):
    """ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¨ ìƒ‰ìƒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸:")
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = model.predict(X_val[:100])  # 100ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    
    # ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„
    unique_colors = set()
    color_distances = []
    
    for pred in predictions:
        # 5ê°œ ìƒ‰ìƒìœ¼ë¡œ ì¬êµ¬ì„±
        colors = pred.reshape(5, 3)
        
        for color in colors:
            # RGBë¥¼ HEXë¡œ ë³€í™˜
            hex_color = rgb_to_hex(color[0], color[1], color[2])
            unique_colors.add(hex_color)
        
        # ìƒ‰ìƒ ê°„ ê±°ë¦¬ ê³„ì‚°
        for i in range(5):
            for j in range(i+1, 5):
                dist = np.linalg.norm(colors[i] - colors[j])
                color_distances.append(dist)
    
    avg_distance = np.mean(color_distances)
    print(f"   ì˜ˆì¸¡ëœ ê³ ìœ  ìƒ‰ìƒ ìˆ˜: {len(unique_colors)}ê°œ")
    print(f"   í‰ê·  ìƒ‰ìƒ ê°„ ê±°ë¦¬: {avg_distance:.3f}")
    print(f"   ìƒ‰ìƒ ë‹¤ì–‘ì„± ì ìˆ˜: {len(unique_colors) / 100 * 100:.1f}%")

def rgb_to_hex(r, g, b):
    """RGB ê°’ì„ HEXë¡œ ë³€í™˜"""
    def to_hex(n):
        hex_val = hex(int(n * 255))[2:]
        return hex_val if len(hex_val) == 2 else f"0{hex_val}"
    
    return f"#{to_hex(r)}{to_hex(g)}{to_hex(b)}"

def save_diverse_model_as_tfjs(model, scaler):
    """ë‹¤ì–‘í•œ ëª¨ë¸ì„ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    script_dir = os.path.dirname(os.path.abspath(__file__))
    model_dir = os.path.join(script_dir, "..", "public", "models", "diverse-face-to-color")
    model_dir = os.path.normpath(model_dir)
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ í›„ ì¬ìƒì„±
    import shutil
    if os.path.exists(model_dir):
        shutil.rmtree(model_dir)
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"ğŸ”„ ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ì„ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì¤‘...")
    
    # 1. ëª¨ë¸ êµ¬ì¡°ë¥¼ TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥
    model_topology = model.to_json()
    model_config = json.loads(model_topology)
    
    # InputLayerì˜ batch_shapeë¥¼ inputShapeë¡œ ë³€í™˜
    if 'config' in model_config and 'layers' in model_config['config']:
        for layer in model_config['config']['layers']:
            if (layer.get('module') == 'keras.layers' and 
                layer.get('class_name') == 'InputLayer'):
                layer_config = layer.get('config', {})
                if 'batch_shape' in layer_config:
                    batch_shape = layer_config['batch_shape']
                    if batch_shape and len(batch_shape) > 1:
                        layer_config['inputShape'] = batch_shape[1:]
                    del layer_config['batch_shape']
    
    # 2. ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì €ì¥
    weights = model.get_weights()
    weight_files = []
    
    for i, weight in enumerate(weights):
        weight_file = f"weights_{i}.bin"
        weight_path = os.path.join(model_dir, weight_file)
        weight.astype('float32').tofile(weight_path)
        weight_files.append(weight_file)
    
    # 3. ê°€ì¤‘ì¹˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
    weights_manifest = [{
        "paths": weight_files,
        "weights": []
    }]
    
    # ê° ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ì¶”ê°€
    weight_names = []
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            weight_names.append(f"{layer.name}/kernel")
            if layer.use_bias:
                weight_names.append(f"{layer.name}/bias")
        elif hasattr(layer, 'gamma'):
            # BatchNormalization ë ˆì´ì–´
            weight_names.extend([
                f"{layer.name}/gamma",
                f"{layer.name}/beta", 
                f"{layer.name}/moving_mean",
                f"{layer.name}/moving_variance"
            ])
    
    for i, weight in enumerate(weights):
        weight_name = weight_names[i] if i < len(weight_names) else f"weight_{i}"
        weights_manifest[0]["weights"].append({
            "name": weight_name,
            "shape": list(weight.shape),
            "dtype": "float32"
        })
    
    # 4. model.json íŒŒì¼ ìƒì„±
    model_json = {
        "modelTopology": model_config,
        "weightsManifest": weights_manifest
    }
    
    with open(os.path.join(model_dir, 'model.json'), 'w') as f:
        json.dump(model_json, f, indent=2)
    
    # 5. ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì €ì¥ (ì •ê·œí™”ê°€ ì ìš©ëœ ê²½ìš°ì—ë§Œ)
    if scaler is not None:
        scaler_info = {
            'mean': scaler.mean_.tolist(),
            'scale': scaler.scale_.tolist(),
            'n_features_in_': scaler.n_features_in_
        }
        
        with open(os.path.join(model_dir, 'scaler_info.json'), 'w') as f:
            json.dump(scaler_info, f, indent=2)
    else:
        # ì •ê·œí™”ê°€ ì´ë¯¸ ì ìš©ëœ ê²½ìš°
        scaler_info = {
            'preprocessing': 'already_normalized',
            'face_descriptor_range': [-0.35, 0.35],
            'physical_features_range': [0.0, 1.0],
            'random_seed_range': [0.0, 1.0],
            'description': 'ë°ì´í„°ê°€ ì´ë¯¸ ì‹¤ì œ íŠ¹ì„±ì— ë§ê²Œ ì •ê·œí™”ë¨'
        }
        
        with open(os.path.join(model_dir, 'scaler_info.json'), 'w') as f:
            json.dump(scaler_info, f, indent=2)
    
    # 6. ëª¨ë¸ ì •ë³´ ì €ì¥
    model_info = {
        'input_dim': 148,
        'output_dim': 15,
        'description': 'ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸: MBTI ì˜ˆì¸¡ì„ ìœ„í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´',
        'input_breakdown': {
            'face_descriptor': 128,
            'physical_features': 15,
            'random_seed': 5,
            'total': 148
        },
        'features': [
            'diverse_color_palettes', 'face_shape_analysis', 'color_harmony',
            'brightness_variation', 'saturation_diversity', 'temperature_range'
        ],
        'color_categories': [
            'vibrant', 'harmonious', 'cool', 'warm', 'neutral', 'contrast', 'random'
        ],
        'mbti_optimized': True
    }
    
    with open(os.path.join(model_dir, 'model_info.json'), 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ì´ TensorFlow.js í˜•ì‹ìœ¼ë¡œ {model_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if scaler is not None:
        print(f"   ğŸ“ ìƒì„±ëœ íŒŒì¼: model.json, {len(weight_files)}ê°œ ê°€ì¤‘ì¹˜ íŒŒì¼, scaler_info.json, model_info.json")
    else:
        print(f"   ğŸ“ ìƒì„±ëœ íŒŒì¼: model.json, {len(weight_files)}ê°œ ê°€ì¤‘ì¹˜ íŒŒì¼, scaler_info.json (ì •ê·œí™” ì •ë³´), model_info.json")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë‹¤ì–‘í•œ ì–¼êµ´-ìƒ‰ìƒ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸ¨ MBTI ì˜ˆì¸¡ì„ ìœ„í•œ ìƒ‰ìƒ ë‹¤ì–‘ì„± í™•ë³´")
    print("ğŸ“Š 148ì°¨ì› ì…ë ¥: descriptor(128) + ë¬¼ë¦¬ì  íŠ¹ì§•(15) + ëœë¤ ì‹œë“œ(5)")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        X, y, metadata = load_diverse_face_color_data()
        
        # ìƒ‰ìƒ ë‹¤ì–‘ì„± ë¶„ì„
        analyze_color_diversity(y, metadata)
        
        # ëª¨ë¸ í•™ìŠµ (ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜)
        model, history, X_val, y_val = train_diverse_face_to_color_model(X, y, metadata)
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        performance = evaluate_model_performance(model, X_val, y_val, metadata)
        
        # TensorFlow.js í˜•ì‹ìœ¼ë¡œ ì €ì¥
        save_diverse_model_as_tfjs(model, None)  # ì •ê·œí™”ëŠ” ì´ë¯¸ ì ìš©ë¨
        
        print("\nğŸ‰ ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
        print("ğŸŒ ì´ì œ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: MSE={performance['mse']:.6f}, ê·¹ê°’ ì •í™•ë„={performance['extreme_accuracy']:.3f}")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("   - model.json: TensorFlow.js í˜¸í™˜ ëª¨ë¸ êµ¬ì¡°")
        print("   - weights_*.bin: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ë“¤")
        print("   - model_info.json: ëª¨ë¸ ì •ë³´")
        
    except FileNotFoundError:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € 'npm run generate-diverse-data'ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
    except Exception as error:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error}")

if __name__ == "__main__":
    main()